# Glossary of Terms
####  [A](#A) | [B](#B) | [C](#C) | [D](#D) | [E](#E) | [F](#F) | [G](#G) | [H](#H) | [I](#I) | [J](#J) | [K](#K) | [L](#L) | [M](#M) | [N](#N) | [O](#O) | [P](#P) | [Q](#Q) | [R](#R) | [S](#S) | [T](#T) | [U](#U) | [V](#V) | [W](#W) | [X](#X) | [Y](#Y) | [Z](#Z)


# A
- Activation function
  - An activation function is a mathematical operation applied to the output of a neuron in a neural network. It determines whether the neuron should be activated or not based on the weighted sum of its inputs. Activation functions introduce non-linearity, allowing neural networks to learn complex patterns and relationships in data. Common activation functions include Sigmoid, Hyperbolic Tangent (tanh), Rectified Linear Unit (ReLU), Leaky ReLU, and Softmax.
# B
- backpropagation
  - Backpropagation, short for "backward propagation of errors," is a supervised learning algorithm used in the training of artificial neural networks. It involves the iterative adjustment of the network's weights based on the difference between the predicted output and the actual target values. The process starts with a forward pass, where input data is fed through the network to generate predictions. The error is then calculated by comparing these predictions to the actual targets. In the backward pass, the error is propagated backward through the network, and the weights are updated using optimization techniques, such as gradient descent, to minimize the error. Backpropagation enables neural networks to learn and improve their performance by adjusting their internal parameters during the training process.
# C
- convolution
  - convolution is a mathematical way of combining two signals to form a third signal.
- CNN
  - A specialized type of neural network designed for visual data processing, leveraging convolutional layers to automatically learn hierarchical features from images, making it effective for tasks like image classification and object detection.
- Clustering
  - A machine learning technique that involves grouping similar data points together based on certain features or characteristics, facilitating pattern discovery and analysis within datasets.
- cv2 
  - cv2 is a powerful library for working with images in Python.
# D
- data leakage
  - The inadvertent inclusion of information in the training process that should only be available during the testing phase, often resulting in over-optimistic model performance estimates due to the unintentional exposure to future information.
- Decision Trees
  - Tree-like models that represent decisions and their potential consequences, where each node corresponds to a decision based on a specific feature, commonly used in classification and regression tasks to create interpretable and easily understandable models.
- dense layer
  - A fully connected layer in a neural network where each neuron is connected to every neuron in the adjacent layers, allowing for complex pattern recognition and feature learning in various machine learning tasks.
- Deep Neural Networks
  - A sophisticated neural network architecture with multiple hidden layers, enabling the model to automatically learn intricate representations and hierarchies of features from the input data, often used for complex tasks like image recognition, natural language processing, and more.
- differential calculus
  - TBD
# E
- epoch
  - An epoch is when all the training data is used at once and is defined as the total number of iterations of all the training data in one cycle for training the machine learning model. 
# F
- filter 
  - A filter, or kernel, in a CNN is a small matrix of weights that slides over the input data (such as an image), performs element-wise multiplication with the part of the input it is currently on.
# G
- Gini index
  - TBD
- Gradient Descent
  - TBD
# H
- H20
  - TBD
- hyper parameters
  - TBD
# I
# J
- jupyter
  - Jupyter is an open-source project that provides a web-based interactive computing platform. It supports various programming languages, but it's most commonly used with languages like Python, R, and Julia.
  - The name "Jupyter" is derived from the three core programming languages it initially supported: Julia, Python, and R.
# K
- keras
  - Keras is a high-level neural network library that runs on top of TensorFlow.
- kernel
  - A filter is a collection of kernels, although the ml domain may use the two terms filter and kernel interchangeably.
# L
- Linear Regression 
  - TBD
- learnability
  - TBD
- Logistic Regression
  - TBD
- Loss Function
  - TBD

# M
  - Multiple Regression 
    - TBD
- MLP 
  - TBD
- MNIST 
  - TBD
# N
- NumPy
  - NumPy is a Python library used for working with arrays. It also has functions for working in domain of linear algebra, fourier transform, and matrices.
# O
- optimizer 
  - TBD
- overfitting
  - TBD
# P
- pandas
  - TBD
- Principal Component Analysis
  - TBD
- Perceptrons
  - TBD
- pooling
  - TBD
- Plotly
  - TBD
# Q
# R
- Recommendation Systems
  - TBD 
- relu (rectified linear unit)
  - TBD 

# S
- Scikit Learn
  - TBD
- Seaborn
  - TBD 
- Support Vector Machines
  - TBD 
- stochastic gradient descent (sgd)
  - SGD is an iterative optimization algorithm that aims to minimize a cost function by updating the model parameters in the opposite direction of the gradient of the cost function. 
- Sparse Categorical Crossentropy (SCC)
  - TBD
- Sigmoid function 
  - TBD
- Spyder
  - A free and open source scientific environment written in Python.
# T
- tableau
  - TBD
- Tensor
  - A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array. A vector is a one dimensional or first order tensor and a matrix is a two dimensional or second order tensor.
- tensorflow
  - TensorFlow is an end-to-end open source platform for machine learning.
- trigonometric functions (cos ...)
  - TBD
# U
- universality 
  - TBD
- Underfitting 
  - TBD

# V
# W
# X
# Y
# Z
